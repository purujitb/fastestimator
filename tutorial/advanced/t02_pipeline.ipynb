{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial 2: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will discuss the following topics:\n",
    "\n",
    "* [Iterating Through Pipeline](#ta02itp)\n",
    "    * [Basic Concept](#ta02bc)\n",
    "    * [Example](#ta02example)\n",
    "* [Dropping Last Batch](#ta02dlb)\n",
    "* [Padding Batch Data](#ta02pbd)\n",
    "* [Benchmark Pipeline Speed](#ta02bps)\n",
    "\n",
    "In the [Beginner Tutorial 4](../beginner/t04_pipeline.ipynb), we learned how to build a data pipeline that handles data loading and preprocessing tasks efficiently. Now that you have understood some basic operations in the `Pipeline`, we will demonstrate some advanced concepts and how to leverage them to create efficient `Pipelines` in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02itp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Through Pipeline\n",
    "\n",
    "In many deep learning tasks, the parameters for preprocessing tasks are precomputed by looping through the dataset. For example, in the `ImageNet` dataset, people usually use a precomputed global pixel average for each channel to normalize the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02bc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how to iterate through the pipeline in FastEstimator. First we will create a sample NumpyDataset from the data dictionary and load it into a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastestimator.dataset.data import cifar10\n",
    "    \n",
    "# sample numpy array to later create datasets from them\n",
    "x_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1)))\n",
    "train_data = {\"x\": x_train, \"y\": y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.numpy_dataset import NumpyDataset\n",
    "\n",
    "# create NumpyDataset from the sample data\n",
    "dataset_fe = NumpyDataset(train_data)\n",
    "\n",
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the loader object for the `Pipeline`, then iterate through the loader with a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[0.5085, 0.0519],\n",
      "        [0.8167, 0.1808],\n",
      "        [0.9175, 0.5209]], dtype=torch.float64), 'y': tensor([[0.5407],\n",
      "        [0.7994],\n",
      "        [0.4728]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.9302, 0.6404],\n",
      "        [0.1795, 0.1212],\n",
      "        [0.8716, 0.9381]], dtype=torch.float64), 'y': tensor([[0.4747],\n",
      "        [0.4103],\n",
      "        [0.3916]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.9929, 0.9415],\n",
      "        [0.6404, 0.8039],\n",
      "        [0.1624, 0.9285]], dtype=torch.float64), 'y': tensor([[0.1118],\n",
      "        [0.8162],\n",
      "        [0.7057]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.0748, 0.8554]], dtype=torch.float64), 'y': tensor([[0.2276]], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "loader_fe = pipeline_fe.get_loader(mode=\"train\")\n",
    "\n",
    "for batch in loader_fe:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02example'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have CIFAR-10 dataset and we want to find global average pixel value over three channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import cifar10\n",
    "\n",
    "cifar_train, _ = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the `batch_size` 64 and load the data into `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cifar = fe.Pipeline(train_data=cifar_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iterate through batch data and compute the mean pixel values for all three channels of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_fe = pipeline_cifar.get_loader(mode=\"train\", shuffle=False)\n",
    "mean_arr = np.zeros((3))\n",
    "for i, batch in enumerate(loader_fe):\n",
    "    mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2))\n",
    "mean_arr = mean_arr / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pixel value over the channels are:  [125.32287898 122.96682199 113.8856495 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean pixel value over the channels are: \", mean_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02dlb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Last Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the total number of dataset elements is not divisible by the `batch_size`, by default, the last batch will have less data than other batches.  To drop the last batch we can set `drop_last` to `True`. Therefore, if the last batch is incomplete it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02pbd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Batch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be scenario where the input tensors have different dimensions within a batch. For example, in Natural Language Processing, we have input strings with different lengths. For that we need to pad the data to the maximum length within the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To further illustrate in code, we will take numpy array that contains different shapes of array elements and load it into the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numpy arrays with different shapes\n",
    "elem1 = np.array([4, 5])\n",
    "elem2 = np.array([1, 2, 6])\n",
    "elem3 = np.array([3])\n",
    "\n",
    "# create train dataset\n",
    "x_train = np.array([elem1, elem2, elem3])\n",
    "train_data = {\"x\": x_train}\n",
    "dataset_fe = NumpyDataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set any `pad_value` that we want to append at the end of the tensor data. `pad_value` can be either `int` or `float`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, pad_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the batch data after padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[4, 5, 0],\n",
      "        [1, 2, 6],\n",
      "        [3, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for elem in iter(pipeline_fe.get_loader(mode='train', shuffle=False)):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02bps'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Pipeline Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that the bottleneck of deep learning training is the data pipeline. As a result, the GPU may be underutilized. FastEstimator provides a method to check the speed of a `Pipeline` in order to help diagnose any potential problems. The way to benchmark `Pipeline` speed in FastEstimator is very simple: call `Pipeline.benchmark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, we will create a `Pipeline` for the CIFAR-10 dataset with list of Numpy operators that expand dimensions, apply `Minmax` and finally `Rotate` the input images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.numpyop.univariate import Minmax, ExpandDims\n",
    "from fastestimator.op.numpyop.multivariate import Rotate\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=cifar_train,\n",
    "                       ops=[Minmax(inputs=\"x\", outputs=\"x_out\"),\n",
    "                            Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180),\n",
    "                            ExpandDims(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")],\n",
    "                       batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark the pre-processing speed for this pipeline in training mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Step: 100, Epoch: 1, Steps/sec: 355.0314672561461\n",
      "FastEstimator: Step: 200, Epoch: 1, Steps/sec: 688.959209809261\n",
      "FastEstimator: Step: 300, Epoch: 1, Steps/sec: 646.3625572114369\n",
      "FastEstimator: Step: 400, Epoch: 1, Steps/sec: 709.4726870671318\n",
      "FastEstimator: Step: 500, Epoch: 1, Steps/sec: 654.4829388343634\n",
      "FastEstimator: Step: 600, Epoch: 1, Steps/sec: 716.1617101086067\n",
      "FastEstimator: Step: 700, Epoch: 1, Steps/sec: 635.2802801079024\n",
      "\n",
      "Breakdown of time taken by Pipeline Operations (train epoch 1)\n",
      "Op         : Inputs : Outputs :  Time\n",
      "--------------------------------------\n",
      "Minmax     : x      : x_out   : 39.42%\n",
      "Rotate     : x_out  : x_out   : 49.45%\n",
      "ExpandDims : x_out  : x_out   : 11.13%\n"
     ]
    }
   ],
   "source": [
    "pipeline.benchmark(mode=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
