{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Getting Started\n",
    "\n",
    "## Overview\n",
    "Welcome to FastEstimator! In this tutorial we are going to cover:\n",
    "* [The three main APIs of FastEstimator: `Pipeline`, `Network`, `Estimator`](#t01ThreeMain)\n",
    "* [An image classification example](#t01ImageClassification)\n",
    "    * [Pipeline](#t01Pipeline)\n",
    "    * [Network](#t01Network)\n",
    "    * [Estimator](#t01Estimator)\n",
    "    * [Training](#t01Training)\n",
    "    * [Inferencing](#t01Inferencing)\n",
    "* [Related Apphub Examples](#t01Apphub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01ThreeMain'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three main APIs\n",
    "All deep learning training workï¬‚ows involve the following three essential components, each mapping to a critical API in FastEstimator.\n",
    "\n",
    "* **Data pipeline**: extracts data from disk/RAM, performs transformations. ->  `fe.Pipeline`\n",
    "\n",
    "\n",
    "* **Network**: performs trainable and differentiable operations. ->  `fe.Network`\n",
    "\n",
    "\n",
    "* **Training loop**: combines the data pipeline and network in an iterative process. ->  `fe.Estimator`\n",
    "\n",
    "<BR>\n",
    "<BR>\n",
    "Any deep learning task can be constructed by following the 3 main steps:\n",
    "<img src=\"../resources/t01_api.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01ImageClassification'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Pipeline\n",
    "We use FastEstimator dataset API to load the MNIST dataset. Please check out [Tutorial 2](./t02_dataset.ipynb) for more details about the dataset API. In this case our data preprocessing involves: \n",
    "1. Expand image dimension from (28,28) to (28, 28, 1) for convenience during convolution operations.\n",
    "2. Rescale pixel values from [0, 255] to [0, 1].\n",
    "\n",
    "Please check out [Tutorial 3](./t03_operator.ipynb) for details about `Operator` and [Tutorial 4](./t04_pipeline.ipynb) for `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import mnist\n",
    "from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n",
    "\n",
    "train_data, eval_data = mnist.load_data()\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       batch_size=32,\n",
    "                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Network\n",
    "\n",
    "The model definition can be either from `tf.keras.Model` or `torch.nn.Module`, for more info about network definitions, check out [Tutorial 5](./t05_model.ipynb). The differentiable operations during training are listed as follows:\n",
    "\n",
    "1. Feed the preprocessed images to the network and get prediction scores.\n",
    "2. Calculate `CrossEntropy` (loss) between prediction scores and ground truth.\n",
    "3. Update the model by minimizing `CrossEntropy`.\n",
    "\n",
    "For more info about `Network` and its operators, check out [Tutorial 6](./t06_network.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.architecture.tensorflow import LeNet\n",
    "# from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model\n",
    "\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n",
    "\n",
    "network = fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n",
    "        UpdateOp(model=model, loss_name=\"ce\") \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Estimator'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Estimator\n",
    "We define the `Estimator` to connect the `Network` to the `Pipeline`, and compute accuracy as a validation metric. Please see [Tutorial 7](./t07_estimator.ipynb) for more about `Estimator` and `Traces`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "import tempfile\n",
    "\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n",
    "          BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n",
    "\n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=2,\n",
    "                         traces=traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \n",
      "FastEstimator-Train: step: 1; ce: 2.2944355; \n",
      "FastEstimator-Train: step: 100; ce: 0.17604804; steps/sec: 724.9; \n",
      "FastEstimator-Train: step: 200; ce: 0.6541523; steps/sec: 755.07; \n",
      "FastEstimator-Train: step: 300; ce: 0.22645846; steps/sec: 793.02; \n",
      "FastEstimator-Train: step: 400; ce: 0.1256088; steps/sec: 773.46; \n",
      "FastEstimator-Train: step: 500; ce: 0.18927144; steps/sec: 809.2; \n",
      "FastEstimator-Train: step: 600; ce: 0.07107867; steps/sec: 779.29; \n",
      "FastEstimator-Train: step: 700; ce: 0.07468874; steps/sec: 806.57; \n",
      "FastEstimator-Train: step: 800; ce: 0.23852134; steps/sec: 781.42; \n",
      "FastEstimator-Train: step: 900; ce: 0.028577618; steps/sec: 826.27; \n",
      "FastEstimator-Train: step: 1000; ce: 0.115206845; steps/sec: 776.94; \n",
      "FastEstimator-Train: step: 1100; ce: 0.07892787; steps/sec: 841.47; \n",
      "FastEstimator-Train: step: 1200; ce: 0.14857067; steps/sec: 791.73; \n",
      "FastEstimator-Train: step: 1300; ce: 0.049252644; steps/sec: 834.86; \n",
      "FastEstimator-Train: step: 1400; ce: 0.046725605; steps/sec: 799.79; \n",
      "FastEstimator-Train: step: 1500; ce: 0.06713241; steps/sec: 812.31; \n",
      "FastEstimator-Train: step: 1600; ce: 0.08489384; steps/sec: 803.99; \n",
      "FastEstimator-Train: step: 1700; ce: 0.00921803; steps/sec: 767.87; \n",
      "FastEstimator-Train: step: 1800; ce: 0.0072177458; steps/sec: 694.9; \n",
      "FastEstimator-Train: step: 1875; epoch: 1; epoch_time: 2.97 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpcxa1xloj/model_best_accuracy.h5\n",
      "FastEstimator-Eval: step: 1875; epoch: 1; ce: 0.06391551; accuracy: 0.9802; since_best_accuracy: 0; max_accuracy: 0.9802; \n",
      "FastEstimator-Train: step: 1900; ce: 0.006937413; steps/sec: 419.42; \n",
      "FastEstimator-Train: step: 2000; ce: 0.10369404; steps/sec: 769.67; \n",
      "FastEstimator-Train: step: 2100; ce: 0.023126157; steps/sec: 787.83; \n",
      "FastEstimator-Train: step: 2200; ce: 0.013664322; steps/sec: 807.29; \n",
      "FastEstimator-Train: step: 2300; ce: 0.15465331; steps/sec: 782.67; \n",
      "FastEstimator-Train: step: 2400; ce: 0.0059421803; steps/sec: 783.07; \n",
      "FastEstimator-Train: step: 2500; ce: 0.03436095; steps/sec: 789.81; \n",
      "FastEstimator-Train: step: 2600; ce: 0.003341827; steps/sec: 813.02; \n",
      "FastEstimator-Train: step: 2700; ce: 0.009203151; steps/sec: 779.41; \n",
      "FastEstimator-Train: step: 2800; ce: 0.0031451974; steps/sec: 818.42; \n",
      "FastEstimator-Train: step: 2900; ce: 0.03497669; steps/sec: 789.2; \n",
      "FastEstimator-Train: step: 3000; ce: 0.0043699713; steps/sec: 816.05; \n",
      "FastEstimator-Train: step: 3100; ce: 0.14205246; steps/sec: 769.89; \n",
      "FastEstimator-Train: step: 3200; ce: 0.00966863; steps/sec: 827.11; \n",
      "FastEstimator-Train: step: 3300; ce: 0.005415355; steps/sec: 780.63; \n",
      "FastEstimator-Train: step: 3400; ce: 0.027803676; steps/sec: 812.07; \n",
      "FastEstimator-Train: step: 3500; ce: 0.3876436; steps/sec: 788.85; \n",
      "FastEstimator-Train: step: 3600; ce: 0.011643453; steps/sec: 809.37; \n",
      "FastEstimator-Train: step: 3700; ce: 0.20535453; steps/sec: 794.13; \n",
      "FastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.46 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpcxa1xloj/model_best_accuracy.h5\n",
      "FastEstimator-Eval: step: 3750; epoch: 2; ce: 0.03874958; accuracy: 0.9867; since_best_accuracy: 0; max_accuracy: 0.9867; \n",
      "FastEstimator-Finish: step: 3750; total_time: 8.86 sec; model_lr: 0.001; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Inferencing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing\n",
    "After training, we can do inferencing on new data with `Pipeline.transform` and `Netowork.transform`. Please checkout [Tutorial 8](./t08_mode.ipynb) for more details. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth class is 7\n",
      "Predicted class is 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANsAAAD3CAYAAACU7SENAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAHcUlEQVR4nO3d38vedR3H8fenhrCDJU1QRJhuIaIGNmF6dytEsPAgwk480EkL+hOKzBAaSSd1JBR0kOiBEHQyxSDGYFmebDZyQTuQ4SYaOXBQDmXK2j4d3Hdk0ue723vudV3Xvcfj7L7e1777wPbcZ/f3x3213nsBV95nZr0AuFqIDULEBiFigxCxQYjYIERsECI2CBEbhIgNQsQGIWKDELFBiNggRGxzrrX2xdbaudZab639aOJ9P159z4ettZ3JNbI2Yptzvfe/VtV3V798orX25Y+/p7W2XFU/XP3yB733V1PrY+2ah0cXQ2ttf1V9s6pOVdWXeu9nV1/fUlV/qartVfW7qvp694c6l+xsi+M7VfW3WonqFx95/eerr52uqr1Cm192tgXSWvtKVR2qlX8k91TV+ar6TVX1qnqg935whsvjEsS2YFprT1bVE1V1tqouVNXnq+pnvffvz3RhXJLYFkxr7bNV9ceqWl596U9VdV/v/fzsVsVa+J5twfTeL1TVkY+8dFRoi8HOtmBaa1+rqgNV1T7y8jd677+d0ZJYI7EtkNbaDbVymv+Gqvp1VX1YVd+uqjNVdVfv/e+zWx2XIrYF0VprtXId7YGqerOq7qqqf1XVsar6QlX9vqp2994vzmqNTPM92+L4Xq2EdrGqvtV7/2fv/b2qerRWovtq/fcuEuaQ2BZAa21XVf1k9cuf9t7/8J9Z7/1wVT25+uW+1tp96fWxNv4bOedaa5+rqlerakdV/bmqlj5+9vFjlwPerJXbuf6RXivT7Gzz75e1Etq5qtrz/07zr14OeLRWLnRvq6pfRVfImtjZIMTOBiFigxCxQYjYIERsECI2CNn0Cd7rGgFMa1NDOxuEiA1CxAYhYoMQsUGI2CBEbBAiNggRG4SIDULEBiFigxCxQYjYIERsECI2CBEbhIgNQsQGIWKDELFBiNggRGwQIjYIERuEiA1CxAYhYoMQsUGI2CBEbBAiNggRG4SIDULEBiFigxCxQYjYIERsECI2CBEbhIgNQsQGIWKDELFBiNggRGwQIjYI2TTrBVwJhw8fHs6eeuqp4eymm26aPO7mzZuHs7179w5nW7duXdeMjcXOBiFigxCxQYjYIERsECI2CGm997W+d81vnLXbbrttODtx4kRwJSuuvfba4WxpaSm4ktm55ZZbhrPHH398ONu2bdsVWM0V06aGdjYIERuEiA1CxAYhYoMQsUGI2CBkQz5i8/zzzw9nx44dG87uvPPOyeMeP358ODty5Mhw9sILLwxnBw4cGM62b98+nJ06dWo4uxybNo3/Stx4443D2VtvvbXu33PqGtxjjz227uPOGzsbhIgNQsQGIWKDELFBiNggZEM+YjNvPvjgg+HsjTfeGM6mTv2fPHnycpY0dM011wxnU6f+p9ZaVfXOO+8MZ/v37x/OHnzwwcnjzhmP2MA8EBuEiA1CxAYhYoMQsUGIU/+s2dSTDcvLy5O/9p577hnODh06NJxNfZjJHHLqH+aB2CBEbBAiNggRG4SIDUKc+ud/vP/++8PZrbfeOpy9/fbbk8ed+pzze++999ILWwxO/cM8EBuEiA1CxAYhYoMQsUHIhvxZ/6zfs88+O5ydPn16OLvuuusmj3vzzTevd0kbhp0NQsQGIWKDELFBiNggRGwQ4q7/q9Drr78+nN1xxx3D2fnz54ez1157bfL3nHpiYANx1z/MA7FBiNggRGwQIjYIERuEiA1CPGJzFXrxxReHs6lraQ899NBwtmPHjsta09XAzgYhYoMQsUGI2CBEbBAiNgjxiM0GNXUKf/fu3cPZK6+8MpwdP358OHPqv6o8YgPzQWwQIjYIERuEiA1CxAYh7vrfoJ5++unh7OWXXx7OHnnkkeHM6f3LY2eDELFBiNggRGwQIjYIERuEuOt/QR07dmxyvmvXruFsy5Ytw9nRo0eHM6f+L8ld/zAPxAYhYoMQsUGI2CBEbBDirv85du7cueHs4Ycfnvy1Fy5cGM727NkznDm9f+XY2SBEbBAiNggRG4SIDULEBiFigxCP2MzYxYsXh7OpD8B46aWXJo97++23D2dTP11r69atk8dlkkdsYB6IDULEBiFigxCxQYjYIMSp/xk7c+bMcHb99dev+7hTPyXr7rvvXvdxmeTUP8wDsUGI2CBEbBAiNggRG4T46VoB77777nC2tLS0rmM+99xzk/OdO3eu67hcOXY2CBEbhIgNQsQGIWKDELFBiFP/Ac8888xwdvLkyXUd8/7775+ctzZ5AzozYGeDELFBiNggRGwQIjYIERuEOPX/KTlx4sRwtm/fvtxCmFt2NggRG4SIDULEBiFigxCxQYhT/5+SqY/OPXv27LqOOfVRvZs3b17XMZkdOxuEiA1CxAYhYoMQsUGI2CBEbBDiOtuMLS8vD2cHDx4czlxnWzx2NggRG4SIDULEBiFigxCxQUjrva/1vWt+I1ylJj/NxM4GIWKDELFBiNggRGwQIjYI+SR3/fuQZrgMdjYIERuEiA1CxAYhYoMQsUGI2CBEbBAiNgj5N7F6Zw7OQ6XXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 200x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = eval_data[0]\n",
    "data = pipeline.transform(data, mode=\"eval\")\n",
    "data = network.transform(data, mode=\"eval\")\n",
    "\n",
    "print(\"Ground truth class is {}\".format(data[\"y\"][0]))\n",
    "print(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"])))\n",
    "img = fe.util.ImgData(x=data[\"x\"])\n",
    "fig = img.paint_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Apphub'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apphub Examples\n",
    "You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:\n",
    "\n",
    "* [MNIST](../../apphub/image_classification/mnist/mnist.ipynb)\n",
    "* [DNN](../../apphub/tabular/dnn/dnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
