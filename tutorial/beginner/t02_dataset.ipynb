{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Creating a FastEstimator dataset\n",
    "\n",
    "## Overview\n",
    "In this tutorial we are going to cover three different ways to create a Dataset using FastEstimator. This tutorial is structured as follows:\n",
    "\n",
    "* [Torch Dataset Recap](#t02Recap)\n",
    "* [FastEstimator Dataset](#t02FEDS)\n",
    "    * [Dataset from disk](#t02Disk)\n",
    "        * [LabeledDirDataset](#t02LDirDs)\n",
    "        * [CSVDataset](#t02CSVDS)\n",
    "    * [Dataset from memory](#t02Memory)\n",
    "        * [NumpyDataset](#t02Numpy)\n",
    "    * [Dataset from generator](#t02Generator)\n",
    "* [Related Apphub Examples](#t02Apphub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02Recap'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Torch Dataset Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset in FastEstimator is a class that wraps raw input data and makes it easier to ingest into your model(s). In this tutorial we will learn about the different ways we can create these Datasets.\n",
    "\n",
    "The FastEstimator Dataset class inherits from the PyTorch Dataset class which provides a clean and efficient interface to load raw data. Thus, any code that you have written for PyTorch will continue to work in FastEstimator too. For a refresher on PyTorch Datasets you can go [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "In this tutorial we will focus on two key functionalities that we need to provide for the Dataset class. The first one is the ability to get an individual data entry from the Dataset and the second one is the ability to get the length of the Dataset. This is done as follows:\n",
    "\n",
    "* len(dataset) should return the size (number of samples) of the dataset.\n",
    "* dataset[i] should return the i-th sample in the dataset. The return value should be a dictionary with data values keyed by strings.\n",
    "\n",
    "Let's create a simple PyTorch Dataset which shows this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([0.77730671, 0.99536305, 0.30362685, 0.82398129, 0.87116199]), 'y': 0.9211995152006527}\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return self.data['x'].shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: self.data[key][idx] for key in self.data}\n",
    "\n",
    "a = {'x': np.random.rand(100,5), 'y': np.random.rand(100)}\n",
    "ds = mydataset(a)\n",
    "print(ds[0])\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02FEDS'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastEstimator Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will showcase how a Dataset can be created using FastEstimator. This tutorial shows three ways to create Datasets. The first uses data from disk, the second uses data already in memory, and the third uses a generator to create a Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02Disk'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will showcase two ways to create a Dataset from disk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02LDirDs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 LabeledDirDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase this we will first have to create a dummy directory structure representing the two classes. Then we create a few files in each of the directories. The following image shows the hierarchy of our temporary data directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/t02_dataset_folder_structure.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data according to the directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import fastestimator as fe\n",
    "\n",
    "tmpdirname = tempfile.mkdtemp()\n",
    "\n",
    "a_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\n",
    "b_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\n",
    "\n",
    "a1 = open(os.path.join(a_tmpdirname.name, \"a1.txt\"), \"x\")\n",
    "a2 = open(os.path.join(a_tmpdirname.name, \"a2.txt\"), \"x\")\n",
    "\n",
    "b1 = open(os.path.join(b_tmpdirname.name, \"b1.txt\"), \"x\")\n",
    "b2 = open(os.path.join(b_tmpdirname.name, \"b2.txt\"), \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, all you have to do is create a Dataset by passing the dummy directory to the `LabeledDirDataset` class constructor. The following code snippet shows how this can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': '/tmp/tmp4_th3s9a/tmphe1zvp3u/a2.txt', 'y': 1}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dataset = fe.dataset.LabeledDirDataset(root_dir=tmpdirname)\n",
    "\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02CSVDS'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 CSVDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase creating a Dataset based on a CSV file, we now create a dummy CSV file representing information for the two classes. First, let's create the data to be used as input as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "import fastestimator as fe\n",
    "\n",
    "tmpdirname = tempfile.mkdtemp()\n",
    "\n",
    "data = {'x': ['a1.txt', 'a2.txt', 'b1.txt', 'b2.txt'], 'y': [0, 0, 1, 1]}\n",
    "df = pd.DataFrame(data=data)\n",
    "df.to_csv(os.path.join(tmpdirname, 'data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done you can create a Dataset by passing the CSV to the `CSVDataset` class constructor. The following code snippet shows how this can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 'a1.txt', 'y': 0}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dataset = fe.dataset.CSVDataset(file_path=os.path.join(tmpdirname, 'data.csv'))\n",
    "\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02Memory'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a Dataset from data stored in memory. This may be useful for smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02Numpy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 NumpyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have data in memory in the form of a Numpy array, it is easy to convert this data into a FastEstimator Dataset. To accomplish this, simply pass your data dictionary into the `NumpyDataset` class constructor. The following code snippet demonstrates this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import fastestimator as fe\n",
    "\n",
    "(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n",
    "train_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})\n",
    "eval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})\n",
    "\n",
    "print (train_data[0]['y'])\n",
    "print (len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02Generator'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset from Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a Dataset using generators. As an example, we will first create a generator which will generate random input data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inputs():\n",
    "    while True:\n",
    "        yield {'x': np.random.rand(4), 'y':np.random.randint(2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the generator as an argument to the `GeneratorDataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([0.15550239, 0.0600738 , 0.29110195, 0.09245787]), 'y': 1}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.dataset import GeneratorDataset\n",
    "\n",
    "dataset = GeneratorDataset(generator=inputs(), samples_per_epoch=10)\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t02Apphub'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apphub Examples\n",
    "You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:\n",
    "\n",
    "* [UNET](../../apphub/semantic_segmentation/unet/unet.ipynb)\n",
    "* [DCGAN](../../apphub/image_generation/dcgan/dcgan.ipynb)\n",
    "* [Siamese Networks](../../apphub/one_shot_learning/siamese_network/siamese.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
